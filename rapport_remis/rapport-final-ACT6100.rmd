---
title: "rapport - ACT6100"
author: "Gabrielle Dupré, Ionela Tarnovschi et Marianne Pelletier"
date: "05/05/2022"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(mlr)
library(tidyverse)
library(C50)
library(gmodels)
```
### Introduction

Dans le cadre du cours analyse de données en actuariat, notre équipe s’est intéressée à la qualité du vin espagnol. Il s’agit d’un problème de classification où nous avons essayé de diviser la qualité du vin en 3 groupes : satisfaisante, bonne, excellente. Les algorithmes choisis pour décrire le problème sont : kNN et foret aléatoire. Les modèles entrainés en R-studio peuvent prédire à laquelle de ces trois classes appartiendra un échantillon de vin selon ses caractéristiques. Le modèle supervisé, foret aléatoire, prédit le résultat en fonctions des variables explicatives associées au vin. Il apprend à faire cette tâche à partir des données d’entrainement. Le modèle non-supervisé, kNN, il ne s’en sert pas des données d’entrainement, mais plutôt des données non-étiquetées. 

### Les données

La base de données structurées choisie pour l’analyse contient 11 variables et 7500 observations. Les variables prises en considérations dans nos modèles reflètent l’effet sur la qualité du vin. Parmi ces variables, on y trouve 5 qui sont catégorielles et 6 quantitatives. La mesure de la qualité du vin (rating) varie entre 4.2 et 4.9. Les regroupements naturels qu’on trouvent dans les données sont : qualité satisfaisante (4.2-4.4), qualité bonne (4.5-4.7) et qualité excellente (4.8 – 4.9). Cette distribution a été faite afin de maintenir les mêmes proportions dans les données d’entrainement et de test pour l’algorithme foret aléatoire. Les données proviennent de la platforme Kaggle. La motivation de nos modèles est d’analyser quelles variables sont liés à la classification de la qualité du vin. Nous avons l’année de récolte des raisins (year), provenance des raisins (region), la sorte du vin (type), son acidité (acidity) et son épaisseur (body). 

### Statistiques descriptives
La variable year varie entre 1910 et 2021. Il y a 288 données abérantes pour cette variable. La variable region contient 76 catégories. La variable type a 21 modalités. La variable body varie entre 2 et 5, et la variable acidity varie entre 1 et 3. Finalement, la variable price varie entre 4.99 et 3119.08.

### Modèles

Plusieurs modèles peuvent être utilisés pour la classification de données, nous avons choisi les modèles KNN et Radom Forest. Nous avons choisit ces deux modéles principalement du fait qu’ils sont simples et faciles à interpréter tout en restant les deux des modèles robustes. Les deux modèles choisis on aussi l’avantage d’être des méthodes non linéaires, par exemple dans le cas de KNN les frontières, les classes sont plutôt des fonctions lisses qui permet des frontières de décisions complexes et plus spécifiques. Un avantage qu’on aime bien du modèle KNN est que celui-ci ne fait aucune hypothèse sur les données, donc il n’a pas été nécessaire de fournir de paramètres estimés à notre modèle. Le modèle de Random Forest partage plusieurs des avantages qui nous permis de choisir le modèle KNN mais un avantage que le modèle KNN n’a pas est que celui-ci prend en charge les variables catégorielles et la majorité de variables explicatrices étant des variables catégorielles le modèle Random Forest semblait pertinent à tester.

Voici les deux modèles proposés :

Commençons par aller chercher nos données et de les travailler pour obtenir la forme voulue.

```{r}
vins <- read.csv("/Users/gabydupre/Desktop/ACT6100/wines_SPA.csv",sep = ";")

vins$quality <- ifelse(vins$rating < 4.5, "satisfaisant", ifelse(vins$rating < 4.8, "bon", "excellent"))

table(vins$quality)
```

Nous avons donc que la grande majorité de nos vins (91.6%) sont de qualité satisfaisante, 7.8% sont bons alors que seulement environ 0.6% sont considérés excellents.

Il faudra ensuite nettoyer nos données. 

```{r}
vins$year <- as.numeric(vins$year)
summary(vins$year)

```


Le premier objectif sera de se débarasser des 290 NAs de la variable explicative year. Pour ce faire, comme la majorité des vins est plus récente, toutes les variables manquantes se verront attribuer la médiane des années de la récoltes des raisins, soit 2015 (au lieu de la moyenne 2013).

```{r}
vins$year_clean <- ifelse(is.na(vins$year), 2015, vins$year)
summary(vins$year_clean)

```

Les prochaines variables que nous voulons nettoyer sont body et acidity.

```{r}
table(as.factor(vins$acidity), useNA = "ifany")

table(as.factor(vins$body), useNA = "ifany")

(body_clean_data <- vins %>% 
  group_by(type, body, acidity) %>% 
  na.omit() %>% 
  count())

```

Nous pouvons remarquer que pour chaque type de vin, un seul score peut lui être attribué pour le corps et l'acidité. Nous pourrons donc trouver nos valeurs manquantes par le type de vin.

```{r}
vins <- right_join(vins, body_clean_data[, 1:3], by = "type") %>% 
  rename(body_clean = body.y,
         acidity_clean = acidity.y) %>% 
  select(-body.x,
         -acidity.x)
summary(vins$body_clean)
summary(vins$acidity_clean)
```

Notre base de données contient maintenant 6955 observations. Nous avons moins de données puisque toutes celles qui contenaient une valeur manquante pour le type de vin ont été supprimé de base de données.

Notre data est donc prêt a être utilisé dans nos modèles.

### Modèle 1 :KNN

Le premier modèle que nous utiliserons sur nos données est un algorithme KNN. C'est un modèle simple et facile à interpreter, et il s'appliquera bien aux types de données que nous possédons.

La première étape consiste à séparer nos données en deux groupes: les données d'entraînement et les données d'évaluation.

```{r}
set.seed(999)

vins_clean <- as_tibble(vins[, c("num_reviews", "price", "year_clean", "body_clean", "acidity_clean", "quality")])
vins_clean$body_clean <- as.numeric(vins_clean$body_clean)
vins_clean$acidity_clean <- as.numeric(vins_clean$acidity_clean)
vins_clean$quality <- as.factor(vins_clean$quality)

train_sample <- sample(6955, 0.9*6955)
vins_train <- vins_clean[train_sample, ]
vins_test <- vins_clean[- train_sample, ]
```

Vérifions si les deux groupes contiennent environ les mêmes proportions de chaque qualité de vin que notre ensemble de données original.

```{r}
prop.table(table(vins$quality))
prop.table(table(vins_train$quality))
prop.table(table(vins_test$quality))
```

Les proportions sont assez similaires d'un groupe à l'autre.

Ensuite, définissons la tâche.

```{r, warning = FALSE}
vinsTask <- makeClassifTask(data = vins_train, target = "quality")
```

La prochaine étape serait de définir l'apprenant. Pour ce faire, nous auront besoin de k, le nombre "voteurs". Le k optimal peut être trouvé par validation croisée.

```{r, message = FALSE}
knnParamSpace <- makeParamSet(makeDiscreteParam("k", values = 5:20))
gridSearch <- makeTuneControlGrid()
cvForTuning <- makeResampleDesc(method = "RepCV", folds = 10, reps = 20)
tunedK <- tuneParams("classif.knn", task = vinsTask, resampling = cvForTuning, par.set = knnParamSpace, control = gridSearch)
knnTuningData <- generateHyperParsEffectData(tunedK)
```

```{r}
plotHyperParsEffect(knnTuningData, x = "k", y = "mmce.test.mean", plot.type = "line")
```

On obtient que le k optimal est égal à 13. En effet, c'est avec cette valeur que l'erreur moyenne de mauvaise classification est la plus faible.

Maintenant, nous pouvons définir l'apprenant et entraîner notre modèle.

```{r}
knn <- setHyperPars(makeLearner("classif.knn", par.vals = tunedK$x))
knnModel <- train(learner = knn, task = vinsTask)
knnPred <- predict(knnModel, newdata = vins_test)
performance(knnPred, measure = list(mmce, acc))
calculateConfusionMatrix(knnPred, relative = T)
```

Au final, on obtient que le modèle a incorrectement prédit 57 des 696 données, soit environ 8.2%.

### Modèle 2: Arbre aléatoire

Le deuxième modèle que nous appliquerons aux données est celui des arbres de décision. Encore une fois, ce modèle a été choisi puisqu'il est simple et facilement interprétable. L'avantage des arbres aléatoire, est qu'ils prennent en charge les variables explicatives catégorielles, contrairement à d'autres modèles.

```{r}
vins_modele2 <- C5.0(vins_train[-6], vins_train$quality)
vins_pred <- predict(vins_modele2, vins_test)
CrossTable(vins_test$quality, vins_pred, prop.chisq = F, prop.c = F, prop.r = F,
           dnn = c("actual quality", "predicted quality"))
```

Nous avons maintenant un taux d'erreur d'environ 6.6%.

Essayons d'améliorer ce modèle avec un boosting.

```{r}
vins_boost <- C5.0(vins_train[-6], vins_train$quality, trials = 10) 
vins_boost_pred <- predict(vins_boost, vins_test)
CrossTable(vins_test$quality, vins_boost_pred, prop.chisq = F, prop.c = F, prop.r = F,
           dnn = c("actual quality", "predicted quality"))
```

Après boosting, le taux d'erreur baisse légèrement: nous avons 44 mauvaises prédictions sur 969 (6.3%). Par contre, on peut remarquer que le modèle n'a attribué à aucun vin la qualité excellente. 

# comparaison des modèles

|  | Modèle KNN | Modèle Random Forest |
|:--------------------------|:--------------------:|:--------------------:|
|mmce|0.082|0.063|
|acc. (catégorie Bon)|0.9181|0.93822|
|acc. (catégorie Excellent)|0.98994|0.99569|
|acc. (catégorie Satisfaisant)|0.92816|0.93966|

On peut voir que le modèle de Random Forest a un plus grand niveau d’exactitude dans la prédiction de chaque catégorie et aussi dans son exactitude de bien prédire des variables au niveau global (mmce).  La différence entre les deux modèles est en moyenne que de 1-2%, donc en générale les deux modèles sont des bons prédicteurs. On voit que la prédiction des vins dans la catégorie excellente est proche de 100% dans le modèle Random Forest et 99% avec KNN ce qui est dû au fait que le groupe de variable classifier « excellent » dans les données est très petit donc cette catégorie retiendra toujours un certain biais. Nous avons réduit le risque d’overfitting avec la validation croisée de l’hyperparamètre k dans le cas de KNN et similairement le boosting aide aussi à réduire le risque potentiel de overfitting pour Random Forest, mais clairement la grosseur du groupe à un certain impacte sur le résultat final. Malgré cette petite problématique avec le groupe « excellent », les deux modèles proposés sont très bons dans la tache de classification générale et donc on croit que notre choix des modèles est adéquat pour notre problème de classification. 

### Conclusion:

Quelques défis qu’on a dû faire face durant le projet était, la préparation des données, principalement le nettoyage. Il peut s’avérer une tâche difficile lorsqu’on doit prendre la décision de soit conserver ou remplacer des données manquantes, car la perte d’information peu souvent être apporté plusieurs problèmes comme la création d’un biais dans les données et aussi réduire grandement la taille des données d’entrainement, mais remplacer des données manquantes peut s’avérer aussi problématique. Dans notre cas nous avons décidé d’utiliser la médiane pour les années manquantes pour retenir le plus d’information possible malgré le fait qu’on augmentait surement un peu le biais de notre modèle. Puis bien sûr la tache de choisir les modèles, dans un cas de classifications, les techniques supervisées, non-supervisé et semi-superviser sont tous des possibilités, donc il a été difficile de tranché qu’elle approche serait la meilleure parmi les plusieurs choix vus en classe. Puis finalement la problématique mentionnée dans la section modèle, l’inégalité de volumes d’observations entre les trois catégories a posé une problématique d’overfitting dû à un grand biais. Nous avons appris l’importance de bien comprendre la problématique que notre modèle essaye de résoudre et les données que nous avons à notre disposition. Les décisions que nous avons dû faire tout au long du travail ont été faites après avoir bien analysé la base de données et analysé les conséquences de chaque option potentielle pour arriver au meilleur compromis possible. 

Le projet nous demandait de proposer deux modèles, mais comme mentionner plus haut il a plusieurs approches a un problème de classification de données, et donc un des modèles qu’on aurait aimé tester est le K-means clustering. Ce modèle aurait été intéressant, car il nous aurait permis de classifier les données dans trois clusters et voir si le modèle aurait été un modèle performant pour la prédiction de la qualité des vins. Le modèle K-means est un modèle qui est rapide, simple et donc facile d’interprétation ce qui était des points qu’on recherchait dans nos modèles. Nous avons aussi considéré une approche un peu plus générale avec un simple arbre décisionnel, mais le Random Forest avait l’avantage d’être un arbre non corréler ce qui vient réduit le biais du modèle.

### Bibliographie
fedesoriano. (April 2022). Spanish Wine Quality Dataset.  from https://www.kaggle.com/datasets/fedesoriano/spanish-wine-quality-dataset